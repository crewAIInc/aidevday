{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "from typing import Any, List\n",
    "\n",
    "# Create reusable loading animation class\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Importing Crew related components\n",
    "from crewai import LLM\n",
    "\n",
    "# Importing CrewAI Flow related components\n",
    "from crewai.flow import Flow, listen, start, persist, or_, router\n",
    "from crewai.flow.flow import FlowState\n",
    "\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Apply a patch to allow nested asyncio loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadingAnimation:\n",
    "    def __init__(self):\n",
    "        self.stop_event = threading.Event()\n",
    "        self.animation_thread = None\n",
    "\n",
    "    def _animate(self, message=\"Loading\"):\n",
    "        chars = \"/—\\\\|\"\n",
    "        while not self.stop_event.is_set():\n",
    "            for char in chars:\n",
    "                sys.stdout.write('\\r' + message + '... ' + char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if self.stop_event.is_set():\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    def start(self, message=\"Loading\"):\n",
    "        self.stop_event.clear()\n",
    "        self.animation_thread = threading.Thread(target=self._animate, args=(message,))\n",
    "        self.animation_thread.daemon = True\n",
    "        self.animation_thread.start()\n",
    "\n",
    "    def stop(self, completion_message=\"Complete\"):\n",
    "        self.stop_event.set()\n",
    "        if self.animation_thread:\n",
    "            self.animation_thread.join()\n",
    "        print(f\"\\r{completion_message} ✓\")\n",
    "\n",
    "# Use the animation for pip install\n",
    "loader = LoadingAnimation()\n",
    "loader.start(\"Installing\")\n",
    "%pip install -r requirements.txt -q\n",
    "loader.stop(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalFlowState(FlowState):\n",
    "  \"\"\"\n",
    "  State for the conversational flow\n",
    "  \"\"\"\n",
    "  message: str = \"\"\n",
    "  query_result: List[Any] = []\n",
    "  conversation_history: List[Any] = []\n",
    "  step_timings: dict = {}\n",
    "  llm_call_time: float = 0\n",
    "  search_time: float = 0\n",
    "\n",
    "@persist()\n",
    "class ConversationalFlow(Flow[ConversationalFlowState]):\n",
    "  @start()\n",
    "  def start_conversation(self):\n",
    "    print(f\"# Starting conversation\\n\")\n",
    "    self.llm = LLM(model=\"ollama/deepseek-r1:8b\")\n",
    "    # self.llm = LLM(model=\"groq/llama-3.3-70b-versatile\")\n",
    "    # self.llm = LLM(model=\"gpt-4o\")\n",
    "    self.state.step_timings = {}\n",
    "    self.state.llm_call_time = 0\n",
    "    self.state.search_time = 0\n",
    "\n",
    "\n",
    "\n",
    "  @router(or_('start_conversation', 'answer_user_message'))\n",
    "  def listen_for_user_input(self):\n",
    "    start_time = time.time()\n",
    "    message = input(\"Enter your message: \")\n",
    "    if message.lower() == \"exit\":\n",
    "      pass\n",
    "    else:\n",
    "      self.state.message = message\n",
    "      self.state.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "      self.state.step_timings['listen_for_user_input'] = time.time() - start_time\n",
    "      return 'message_received'\n",
    "\n",
    "\n",
    "\n",
    "  @router('message_received')\n",
    "  def process_user_input(self):\n",
    "    start_time = time.time()\n",
    "    messages = self.state.conversation_history.copy()\n",
    "    messages.append(\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"\"\"Check if you need more details about crewai enterprise features to answer.\n",
    "                    Only ask for more info if the question is not clearly about crewai.\n",
    "\n",
    "                    If you have enough info, just reply 'complete'.\n",
    "                    If you need more info, reply with one search sentence.\n",
    "\n",
    "                    Look at our chat history and my message.\n",
    "                    Decide if you can give a good answer with what you know.\"\"\"\n",
    "    })\n",
    "\n",
    "    llm_start = time.time()\n",
    "    response = self.llm.call(messages)\n",
    "    self.state.llm_call_time += time.time() - llm_start\n",
    "\n",
    "    if response == 'complete':\n",
    "      self.state.step_timings['process_user_input'] = time.time() - start_time\n",
    "      return 'answer'\n",
    "    else:\n",
    "      # search_start = time.time()\n",
    "      # client = OpenAI()\n",
    "      # embedding = client.embeddings.create(input=response, model=\"text-embedding-3-large\")\n",
    "      # pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "      # index = pc.Index(host=os.getenv('PINECONE_INDEX_HOST'))\n",
    "      # result = index.query(namespace=\"transcripts\", vector=embedding.data[0].embedding, top_k=3, include_metadata=True)\n",
    "      # self.state.search_time += time.time() - search_start\n",
    "\n",
    "      # self.state.query_result = [item['metadata']['text'] for item in result['matches']]\n",
    "      # context = \"\\n\\nAdditional context:\\n\" + \"\\n\".join(self.state.query_result)\n",
    "      # self.state.conversation_history[-1][\"content\"] += context\n",
    "      # self.state.step_timings['process_user_input'] = time.time() - start_time\n",
    "      return 'answer'\n",
    "\n",
    "\n",
    "\n",
    "  @listen('answer')\n",
    "  def answer_user_message(self):\n",
    "    start_time = time.time()\n",
    "    llm_start = time.time()\n",
    "    response = self.llm.call(self.state.conversation_history)\n",
    "    self.state.llm_call_time += time.time() - llm_start\n",
    "\n",
    "    self.state.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    print(f\"# Assistant response: {response}\\n\")\n",
    "    self.state.step_timings['answer_user_message'] = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nTiming Summary:\")\n",
    "    print(f\"Total LLM call time: {self.state.llm_call_time:.2f}s\")\n",
    "    print(f\"Total Search time: {self.state.search_time:.2f}s\")\n",
    "    print(\"Step timings:\")\n",
    "    for step, timing in self.state.step_timings.items():\n",
    "        print(f\"  {step}: {timing:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = ConversationalFlow()\n",
    "flow.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = ConversationalFlow()\n",
    "flow.kickoff(inputs={\"id\": \"4649d966-662f-4414-94de-8cdf0cefb0ff\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
