{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Code Documentation Agents with CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create a crew of multiple Agents for writing documentation using CrewAI Flows.\n",
    "\n",
    "The crew will analyze code from any public GitHub repository and generate comprehensive documentation\n",
    "by working collaboratively using specialized agents with different roles and responsibilities.\n",
    "\n",
    "CrewAI Flows enable coordinated execution and communication between agents to produce high-quality\n",
    "documentation for any codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create reusable loading animation class\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class LoadingAnimation:\n",
    "    def __init__(self):\n",
    "        self.stop_event = threading.Event()\n",
    "        self.animation_thread = None\n",
    "\n",
    "    def _animate(self, message=\"Loading\"):\n",
    "        chars = \"/—\\\\|\"\n",
    "        while not self.stop_event.is_set():\n",
    "            for char in chars:\n",
    "                sys.stdout.write('\\r' + message + '... ' + char)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.1)\n",
    "                if self.stop_event.is_set():\n",
    "                    sys.stdout.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    def start(self, message=\"Loading\"):\n",
    "        self.stop_event.clear()\n",
    "        self.animation_thread = threading.Thread(target=self._animate, args=(message,))\n",
    "        self.animation_thread.daemon = True\n",
    "        self.animation_thread.start()\n",
    "\n",
    "    def stop(self, completion_message=\"Complete\"):\n",
    "        self.stop_event.set()\n",
    "        if self.animation_thread:\n",
    "            self.animation_thread.join()\n",
    "        print(f\"\\r{completion_message} ✓\")\n",
    "\n",
    "# Use the animation for pip install\n",
    "loader = LoadingAnimation()\n",
    "loader.start(\"Installing\")\n",
    "%pip install -r requirements.txt -q\n",
    "loader.stop(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Define a fake `load_dotenv` function\n",
    "def _load_dotenv(*args, **kwargs):\n",
    "    env_path = kwargs.get('dotenv_path', '.env')  # Default to '.env'\n",
    "    parsed_env = dotenv_values(env_path)\n",
    "\n",
    "    # Manually set valid key-value pairs\n",
    "    for key, value in parsed_env.items():\n",
    "        if key and value:  # Check for valid key-value pairs\n",
    "            os.environ[key] = value\n",
    "\n",
    "dotenv.load_dotenv = _load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Setup\n",
    "Initial imports for the CrewAI Flow and Crew and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import yaml\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Importing Crew related components\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Importing CrewAI Flow related components\n",
    "from crewai.flow.flow import Flow, listen, start\n",
    "\n",
    "# Apply a patch to allow nested asyncio loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "Prior to getting started, you will need to create API Keys for the NVIDIA API Catalog.\n",
    "\n",
    "- NVIDIA API Catalog\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as llama-3.3-70b-instruct.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already.\n",
    "\n",
    "### Export API Keys\n",
    "\n",
    "Save this API Key as environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_NIM_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_NIM_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "\n",
    "print(f\"NVIDIA API Key: {os.environ['NVIDIA_API_KEY']}\")\n",
    "print(f\"NVIDIA NIM API Key: {os.environ['NVIDIA_NIM_API_KEY']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the NVIDIA API Catalog\n",
    "Let's test the API endpoint. The NVIDIA NIM Microservices used in this notebook are llama-3.3-70b and an NVIDIA text embedding model, nv-embedqa-e5-v5. These are models are defined in the yaml files in the `config` directory if you're looking for them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "loader = LoadingAnimation()\n",
    "loader.start(\"Calling NVIDIA NIM\")\n",
    "\n",
    "llm = LLM(model=\"nvidia_nim/meta/llama-3.3-70b-instruct\")\n",
    "response = llm.call(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's a good name for a dog?\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "loader.stop(\"Complete\")\n",
    "\n",
    "print('\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Locally Run NVIDIA NIM Microservices\n",
    "\n",
    "Once you familiarize yourself with this blueprint, you may want to self-host models with NVIDIA NIM Microservices using NVIDIA AI Enterprise software license. This gives you the ability to run models anywhere, giving you ownership of your customizations and full control of your intellectual property (IP) and AI applications.\n",
    "\n",
    "[Learn more about NIM Microservices](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the project URL\n",
    "\n",
    "In this demo, a sample repository is provided for you. However, feel fry to test this on other public repositories! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_url = \"https://github.com/crewAIInc/crewAI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for Llama 3.3 Prompting Template\n",
    "\n",
    "When using different models the ability to go a lower level and change the prompting template can drastically improve the performance of the model, you want to make sure to watch for the model's training prompt patterns and adjust accordingly.\n",
    "\n",
    "For Meta's Llama you can find it [in here](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#prompt-template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents Prompting Template for Llama 3.3\n",
    "system_template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{{ .System }}<|eot_id|>\"\"\"\n",
    "prompt_template=\"\"\"<|start_header_id|>user<|end_header_id|>{{ .Prompt }}<|eot_id|>\"\"\"\n",
    "response_template=\"\"\"<|start_header_id|>assistant<|end_header_id|>{{ .Response }}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Application Mapper Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class Component(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    file_path: Optional[str] = None\n",
    "\n",
    "class RelevantFile(BaseModel):\n",
    "    path: str\n",
    "    description: str\n",
    "\n",
    "class ProjectReport(BaseModel):\n",
    "    project_overview: str\n",
    "    project_organization: str\n",
    "    main_components: List[Component]\n",
    "    relevant_files: List[RelevantFile]\n",
    "    additional_information: Optional[str] = None\n",
    "\n",
    "\n",
    "class ComponentFile(BaseModel):\n",
    "    path: str\n",
    "    description: str\n",
    "\n",
    "class ComponentDetails(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    files_and_directories: List[ComponentFile]\n",
    "    main_functionality: str\n",
    "    integration: str\n",
    "    additional_info: Optional[str] = None\n",
    "\n",
    "class ComponentsReport(BaseModel):\n",
    "    components: List[ComponentDetails]\n",
    "    other_relevant_info: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    "    DirectorySearchTool\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/application_mapper_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/application_mapper_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "principal_engineer = Agent(\n",
    "  config=agents_config['principal_engineer'],\n",
    "  system_template=system_template,\n",
    "  prompt_template=prompt_template,\n",
    "  response_template=response_template,\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "understand_application = Task(\n",
    "  config=tasks_config['understand_application'],\n",
    "  agent=principal_engineer,\n",
    "  output_pydantic=ProjectReport\n",
    ")\n",
    "\n",
    "map_components = Task(\n",
    "  config=tasks_config['map_components'],\n",
    "  agent=principal_engineer,\n",
    "  output_pydantic=ComponentsReport\n",
    ")\n",
    "\n",
    "application_mapper_crew = Crew(\n",
    "    agents=[principal_engineer],\n",
    "    tasks=[understand_application, map_components],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from crewai.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ComponentInfoInput(BaseModel):\n",
    "    \"\"\"Input schema for ComponentInfoTool.\"\"\"\n",
    "    component_name: str = Field(..., description=\"Name of the component to get information about\")\n",
    "\n",
    "class ComponentInfoTool(BaseTool):\n",
    "    name: str = \"Component Information Tool\"\n",
    "    description: str = \"Gets detailed information about a specific component from the components report\"\n",
    "    args_schema: Type[BaseModel] = ComponentInfoInput\n",
    "    components_report: BaseModel = Field(None, description=\"Components report containing component information\")\n",
    "\n",
    "    def __init__(self, components_report):\n",
    "        super().__init__()\n",
    "        self.components_report = components_report\n",
    "\n",
    "    def _run(self, component_name: str) -> str:\n",
    "        # Find the component in the components array\n",
    "        component = next((c for c in self.components_report.components if c.name.lower() == component_name.lower()), None)\n",
    "\n",
    "        if not component:\n",
    "            return f\"Component '{component_name}' not found\"\n",
    "\n",
    "        # Format component information as string\n",
    "        info = f\"\"\"\n",
    "Component: {component.name}\n",
    "Description: {component.description}\n",
    "Main Functionality: {component.main_functionality}\n",
    "Integration: {component.integration}\n",
    "Additional Info: {component.additional_info}\n",
    "\n",
    "Files and Directories:\n",
    "\"\"\"\n",
    "        for file in component.files_and_directories:\n",
    "            info += f\"- {file.path}: {file.description}\\n\"\n",
    "\n",
    "        return info.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Planning Crew\n",
    "\n",
    "Initial strucutre data we will use to capture the output of the planning crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data structures to capture documentation planning output\n",
    "class DocSection(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    content: str\n",
    "\n",
    "class DocItem(BaseModel):\n",
    "    \"\"\"Represents a documentation item\"\"\"\n",
    "    title: str\n",
    "    sections: list[DocSection]\n",
    "    prerequisites: str\n",
    "    associated_entities: list[str]\n",
    "    examples: list[str]\n",
    "    goal: str\n",
    "\n",
    "class DocPlan(BaseModel):\n",
    "    \"\"\"Documentation plan\"\"\"\n",
    "    docs: list[DocItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/planner_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/planner_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "code_explorer = Agent(\n",
    "  config=agents_config['code_explorer'],\n",
    "  system_template=system_template,\n",
    "  prompt_template=prompt_template,\n",
    "  response_template=response_template,\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "documentation_planner = Agent(\n",
    "  config=agents_config['documentation_planner'],\n",
    "  system_template=system_template,\n",
    "  prompt_template=prompt_template,\n",
    "  response_template=response_template,\n",
    "  tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "  ]\n",
    ")\n",
    "\n",
    "analyze_codebase = Task(\n",
    "  config=tasks_config['analyze_codebase'],\n",
    "  agent=code_explorer\n",
    ")\n",
    "create_documentation_plan = Task(\n",
    "  config=tasks_config['create_documentation_plan'],\n",
    "  agent=documentation_planner,\n",
    "  output_pydantic=DocPlan\n",
    ")\n",
    "\n",
    "planning_crew = Crew(\n",
    "    agents=[code_explorer, documentation_planner],\n",
    "    tasks=[analyze_codebase, create_documentation_plan],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Documentation Crew\n",
    "\n",
    "Crew of AI Agents to execute the documentation plan and create the documentation.\n",
    "Creating a guardrail to check the mermaid syntax in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tasks import TaskOutput\n",
    "import re\n",
    "\n",
    "def check_mermaid_syntax(task_output: TaskOutput):\n",
    "    text = task_output.raw\n",
    "\n",
    "    # Find all mermaid code blocks in the text\n",
    "    mermaid_blocks = re.findall(r'```mermaid\\n(.*?)\\n```', text, re.DOTALL)\n",
    "\n",
    "    for block in mermaid_blocks:\n",
    "        diagram_text = block.strip()\n",
    "        lines = diagram_text.split('\\n')\n",
    "        corrected_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            corrected_line = re.sub(r'\\|.*?\\|>', lambda match: match.group(0).replace('|>', '|'), line)\n",
    "            corrected_lines.append(corrected_line)\n",
    "\n",
    "        text = text.replace(block, \"\\n\".join(corrected_lines))\n",
    "\n",
    "    task_output.raw = text\n",
    "    return (True, task_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    "    WebsiteSearchTool\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/documentation_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/documentation_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "overview_writer = Agent(config=agents_config['overview_writer'], tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool(),\n",
    "    WebsiteSearchTool(\n",
    "      website=\"https://mermaid.js.org/intro/\",\n",
    "      config=dict(\n",
    "        embedder=dict(\n",
    "            provider=\"nvidia\",\n",
    "            config=dict(\n",
    "                model=\"nvidia/nv-embedqa-e5-v5\"\n",
    "            ),\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "])\n",
    "\n",
    "documentation_reviewer = Agent(config=agents_config['documentation_reviewer'], tools=[\n",
    "    DirectoryReadTool(directory=\"docs/\", name=\"Check existing documentation folder\"),\n",
    "    FileReadTool(),\n",
    "])\n",
    "\n",
    "draft_documentation = Task(\n",
    "  config=tasks_config['draft_documentation'],\n",
    "  agent=overview_writer\n",
    ")\n",
    "\n",
    "qa_review_documentation = Task(\n",
    "  config=tasks_config['qa_review_documentation'],\n",
    "  agent=documentation_reviewer,\n",
    "  guardrail=check_mermaid_syntax,\n",
    "  max_retries=5\n",
    ")\n",
    "\n",
    "documentation_crew = Crew(\n",
    "    agents=[overview_writer, documentation_reviewer],\n",
    "    tasks=[draft_documentation, qa_review_documentation],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Documentation Flow\n",
    "\n",
    "A Flow to create the documentation for the project where we will use the planning crew to plan the documentation and the documentation crew to create the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DocumentationState(BaseModel):\n",
    "  \"\"\"\n",
    "  State for the documentation flow\n",
    "  \"\"\"\n",
    "  project_url: str = project_url\n",
    "  repo_path: Path = \"workdir/\"\n",
    "  docs: List[str] = []\n",
    "  project_report: ProjectReport | None = None\n",
    "  components_report: ComponentsReport | None = None\n",
    "\n",
    "class CreateDocumentationFlow(Flow[DocumentationState]):\n",
    "  # Clone the repository, initial step\n",
    "  # No need for AI Agents on this step, so we just use regular Python code\n",
    "  @start()\n",
    "  def clone_repo(self):\n",
    "    print(f\"# Cloning repository: {self.state.project_url}\\n\")\n",
    "    # Extract repo name from URL\n",
    "    repo_name = self.state.project_url.split(\"/\")[-1]\n",
    "    self.state.repo_path = f\"{self.state.repo_path}{repo_name}\"\n",
    "\n",
    "  # Check if directory exists\n",
    "    if Path(self.state.repo_path).exists():\n",
    "      print(f\"# Repository directory already exists at {self.state.repo_path}\\n\")\n",
    "      subprocess.run([\"rm\", \"-rf\", self.state.repo_path])\n",
    "      print(\"# Removed existing directory\\n\")\n",
    "\n",
    "    # Clone the repository\n",
    "    subprocess.run([\"git\", \"clone\", self.state.project_url, self.state.repo_path])\n",
    "    return self.state\n",
    "\n",
    "  @listen(clone_repo)\n",
    "  def map_project_structure(self):\n",
    "    print(f\"# Mapping project structure for: {self.state.repo_path}\\n\")\n",
    "    result = application_mapper_crew.kickoff(inputs={'repo_path': self.state.repo_path})\n",
    "    print(f\"# Project structure for {self.state.repo_path}:\")\n",
    "    for component in result.tasks_output[1].pydantic.components:\n",
    "        print(f\"    - {component.name}\")\n",
    "    self.state.project_report = result.tasks_output[0].pydantic\n",
    "    self.state.components_report = result.tasks_output[1].pydantic\n",
    "\n",
    "  @listen(map_project_structure)\n",
    "  def plan_docs(self):\n",
    "    print(f\"# Planning documentation for: {self.state.repo_path}\\n\")\n",
    "\n",
    "    tool = ComponentInfoTool(components_report=self.state.components_report)\n",
    "    planning_crew.agents[0].tools.append(tool)\n",
    "    planning_crew.agents[1].tools.append(tool)\n",
    "\n",
    "    result = planning_crew.kickoff(inputs={\n",
    "      'repo_path': self.state.repo_path,\n",
    "      'project_overview': self.state.project_report.project_overview,\n",
    "      'project_organization': self.state.project_report.project_organization,\n",
    "      'additional_information': self.state.project_report.additional_information,\n",
    "      'components_names': [c.name for c in self.state.components_report.components]\n",
    "    })\n",
    "    print(f\"# Planned docs for {self.state.repo_path}:\")\n",
    "    for doc in result.pydantic.docs:\n",
    "        print(f\"    - {doc.title}\")\n",
    "    return result\n",
    "\n",
    "  @listen(plan_docs)\n",
    "  def save_plan(self, plan):\n",
    "    with open(\"docs/plan.json\", \"w\") as f:\n",
    "      f.write(plan.raw)\n",
    "\n",
    "  @listen(plan_docs)\n",
    "  def create_docs(self, plan):\n",
    "    for doc in plan.pydantic.docs:\n",
    "      print(f\"\\n# Creating documentation for: {doc.title}\")\n",
    "\n",
    "      # Save documentation to file in docs folder\n",
    "      docs_dir = Path(\"docs\")\n",
    "      docs_dir.mkdir(exist_ok=True)\n",
    "      title = doc.title.lower().replace(\" \", \"_\") + \".mdx\"\n",
    "      documentation = []\n",
    "\n",
    "      with open(docs_dir / title, \"w\") as f:\n",
    "        for section in doc.sections:\n",
    "          print(f\"  ## Writing section: {section.title}\")\n",
    "          retries = 3\n",
    "          for attempt in range(retries):\n",
    "              try:\n",
    "                  result = documentation_crew.kickoff(inputs={\n",
    "                      'title': doc.title,\n",
    "                      'repo_path': self.state.repo_path,\n",
    "                      'project_overview': self.state.project_report.project_overview,\n",
    "                      'project_organization': self.state.project_report.project_organization,\n",
    "                      'additional_information': self.state.project_report.additional_information,\n",
    "                      'section': section.title,\n",
    "                      'description': section.description,\n",
    "                      'prerequisites': doc.prerequisites,\n",
    "                      'associated_entities': doc.associated_entities,\n",
    "                      'examples': '\\n'.join(doc.examples),\n",
    "                      'documentation': '\\n'.join(documentation) if documentation else 'this is the first section',\n",
    "                      'goal': doc.goal\n",
    "                  })\n",
    "                  documentation.append(result.raw)\n",
    "                  break\n",
    "              except Exception as e:\n",
    "                  if attempt == retries - 1:  # Last attempt\n",
    "                      print(f\"Error creating documentation for {section.title} after {retries} attempts: {e}\")\n",
    "                  else:\n",
    "                      print(f\"Attempt {attempt + 1} failed for {section.title}: {e}. Retrying...\")\n",
    "\n",
    "        self.state.docs.append(str(docs_dir / title))\n",
    "        f.write(\"\\n\".join(documentation))\n",
    "    print(f\"\\n# Documentation created for: {self.state.repo_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing helper methods to plot and execute the flow in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the flow\n",
    "flow = CreateDocumentationFlow()\n",
    "flow.plot()\n",
    "\n",
    "# Display the flow visualization using IFrame\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Display the flow visualization\n",
    "IFrame(src='./crewai_flow.html', width='100%', height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Documentation Flow\n",
    "\n",
    "After running this cell, check the `docs` directory for the generated documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = CreateDocumentationFlow()\n",
    "flow.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot One of the Documents\n",
    "\n",
    "Let's visualize one of the generated documentation files to verify the output. This will help us ensure the documentation was created successfully and formatted correctly.\n",
    "\n",
    "The generated documentation files can be found in the `docs` directory in the root of the project. Each documentation file is saved with a `.mdx` extension and follows the naming convention of lowercase words separated by underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in docs folder and display the first doc using IPython.display\n",
    "from IPython.display import Markdown\n",
    "import pathlib\n",
    "\n",
    "docs_dir = pathlib.Path(\"docs\")\n",
    "print(\"Documentation files generated:\")\n",
    "for doc_file in docs_dir.glob(\"*.mdx\"):\n",
    "    print(f\"- docs/{doc_file.name}\")\n",
    "\n",
    "print(\"\\nDisplaying contents of first doc:\\n\")\n",
    "first_doc = pathlib.Path(flow.state.docs[0]).read_text()\n",
    "display(Markdown(first_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
